{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33af0fd-a184-4c92-9eaf-7cff77566bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1993cbbb-23c5-4386-99c4-2f21c23322a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1284e-296d-4c89-9f3f-fa61a313e569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b0767-a91b-40c2-89db-b999b7bae605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59dff806-3c51-48ac-96f0-c1f7c6c681cd",
   "metadata": {},
   "source": [
    "# Conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927df14c-fbc8-4199-81bd-e15f81ba6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05c43939-b32b-47a6-b664-6a7ff28adcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg: \n",
    "\n",
    "    # to store HF pre-trained models weights and configs\n",
    "    HF_CACHE_ROOT = os.path.join(\"..\", \"..\", \"..\",\n",
    "                                 \"data\",\n",
    "                                 \"05_cache\", \n",
    "                                 \"HF\"\n",
    "                                )\n",
    "\n",
    "\n",
    "    # to store HF pre-trained models weights and configs\n",
    "    HF_CACHE_ROOT = os.path.join(\"..\", \"..\", \"..\",\n",
    "                                 \"data\",\n",
    "                                 \"06_fine_tune\",\n",
    "                                 \"01_tuto\",\n",
    "                                 \"01_hug_llm\",\n",
    "                                 \"ch03\",\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60ed72-7ae0-4eab-909e-6bd3c6614811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92623a8-f68e-44a3-b273-df85ffafbfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de7fb4d4-597a-4fb7-ada1-c8d3d6c9442e",
   "metadata": {},
   "source": [
    "# HF Cache management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f4cbb-b230-42f0-b43e-e147f2845750",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/datasets/en/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6744d68-0c2e-4f12-8a78-13b9cafee577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: None\n",
      "HF_HOME: ../../../data/05_cache/HF\n"
     ]
    }
   ],
   "source": [
    "print(\"HF_HOME:\", os.environ.get(\"HF_HOME\"))\n",
    "os.environ[\"HF_HOME\"] = cfg.HF_CACHE_ROOT\n",
    "print(\"HF_HOME:\", os.environ.get(\"HF_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed76039-f911-4345-ac89-06097ad216d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HUB_CACHE: None\n",
      "HF_HUB_CACHE: ../../../data/05_cache/HF\n"
     ]
    }
   ],
   "source": [
    "print(\"HF_HUB_CACHE:\", os.environ.get(\"HF_HUB_CACHE\"))\n",
    "os.environ[\"HF_HUB_CACHE\"] = cfg.HF_CACHE_ROOT\n",
    "print(\"HF_HUB_CACHE:\", os.environ.get(\"HF_HUB_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b985b-7395-4230-abbd-5bf15e15575c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4df68-63c2-487b-bcee-0b5e53d63b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34ac1942-43e5-4732-b9a2-a24fbd7d3eae",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e1f5ac-d89d-4662-983a-389654246bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "#_________\n",
    "import torch\n",
    "\n",
    "#__________\n",
    "from transformers import pipeline\n",
    "\n",
    "#_________\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2233379-95be-4b31-a7ed-f7fcedb89f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99807b-f036-4e37-928a-99ffb5fdb9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3540eec7-d0d3-41ef-889e-a35a2ba0f6fb",
   "metadata": {},
   "source": [
    "# Service Token Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0210827b-75e0-4ac7-8c28-f882ab81b363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token loaded: Yes\n"
     ]
    }
   ],
   "source": [
    "# Verify token is loaded\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN_READ = os.getenv(\"07_FR_phone_TokenType_READ\")\n",
    "print(f\"Token loaded: {'Yes' if HF_TOKEN_READ else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f898023-6199-48c5-bb4a-d45fd542bdf0",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868526a3-8ccd-4f0d-9c1c-0577c0fba42f",
   "metadata": {},
   "source": [
    "## Simple loop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "913a4d66-b446-4635-b140-04cb826c0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13684047-dc8f-42aa-a83e-01490ab0331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8b0fec6-18a8-48fe-bac4-274471821f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eac7fbb3d78444a977f343f5cca05d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "classifier.weight                          | MISSING    | \n",
      "classifier.bias                            | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "# Same as before\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                           token=HF_TOKEN_READ,\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e4f6287-a8a4-402f-8227-674f9f79fde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same as before\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98daee20-b847-4c8d-bb99-6ad85d2a3379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a93375-cb8d-4b5b-9f5c-9429f309e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26d30f33-328c-4ae1-a1c9-bd8453a7a928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9005, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model(**batch).loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c7e1d96-c74c-478d-811e-ee9391bd106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a22e8c-bfc5-4561-b615-9139ff2f924b",
   "metadata": {},
   "source": [
    "Of course, just training the model on two sentences is not going to yield very good results. To get better results, you will need to prepare a bigger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2200f2ac-a071-43ed-a763-e9ea87b233d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d11eac1a-80ee-413f-a877-1a382d7a6c24",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa861c45-0b5e-43b9-91fb-2efd19f209cf",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/datasets/v4.5.0/en/package_reference/loading_methods#datasets.load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c971806a-c24b-4db4-be58-eac32397499c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(path=\"glue\", # is the name of a dataset builder and data_files or data_dir is specified (available builders are “json”, “csv”, “parquet”, “arrow”, “text”, “xml”, “webdataset”, “imagefolder”, “audiofolder”, “videofolder”)\n",
    "                            name=\"mrpc\", # Defining the name of the dataset configuration.\n",
    "                            token=HF_TOKEN_READ,\n",
    "                           )\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48e94207-5931-4291-98aa-92f1b30f723f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8473b5af-d9e8-4cbe-bd5e-18a695dadd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value('string'),\n",
       " 'sentence2': Value('string'),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent']),\n",
       " 'idx': Value('int32')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "298f2ffd-0f67-48a4-9c11-dbe000a3275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['not_equivalent', 'equivalent'])\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_dataset.features[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0acc463-a565-4166-a5c0-087d1cdf77ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fadc9e6-bafc-44e1-8497-c3fd6343c33f",
   "metadata": {},
   "source": [
    "## **Data preparation => tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43f05396-f3ee-411f-8324-397b008a9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(list(raw_datasets[\"train\"][\"sentence1\"]))\n",
    "tokenized_sentences_2 = tokenizer(list(raw_datasets[\"train\"][\"sentence2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "593d2fac-8917-4b29-a06b-4ec4f13f0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dba77a06-eb7c-46bf-9b1a-1bbada7233ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca232c2a4f14bf1978690f9b8f3bf58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the raw dataset that two sentences that needs to be tokenize, using .map in raw_dataset is faster \n",
    "#    because it use Apache arrow under the hood\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f2637-053d-42a2-8f90-6d950b8f0c04",
   "metadata": {},
   "source": [
    "## Dynamic padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669e8b2-9f02-42e9-b912-15f0c4c8e7b2",
   "metadata": {},
   "source": [
    "** padding wasnt done during the padding, it is better to do it latter, when feeding the model, it is call dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0d54ea9-2989-46af-b47f-34b727922d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52c19a3a-78c5-44ed-ace9-813347d07ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28396efb-ac29-4a23-9ff4-09059b15a445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ff045-063c-43c9-ae9e-9fc0f98f2439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68129fc7-0e01-4eef-89ce-b23fc322c775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5172447-44fd-46f1-be9f-5e39b7b31c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce32158c-e53d-4cd7-92aa-4312a563f5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## assuming the sample is a batch of 8 instances for training \n",
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afce31b-513c-46d1-9eaa-75ff40e6ebe7",
   "metadata": {},
   "source": [
    "## Training - Defining TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc0d6e-4ec9-40d9-8859-9314939f89c6",
   "metadata": {},
   "source": [
    "**Defining ROOT where all hyperameters, place where the model will be saved, as well as, checkpoints along the way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "493745cc-0c04-4383-a79e-dbbcc83a42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir_path = os.path.join(cfg.HF_CACHE_ROOT, \"test-trainer\")\n",
    "training_args = TrainingArguments(output_dir=output_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36d34854-66f7-4ffe-9dc3-216059d4febe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../../data/06_fine_tune/01_tuto/01_hug_llm/ch03',)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.HF_CACHE_ROOT,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf362a87-538e-4d5f-b4a0-f4788adb3946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88c4eb09-7384-42ab-acf5-84171ae8c80d",
   "metadata": {},
   "source": [
    "## Training - Selecting different model head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9789b501-f269-47a4-bec5-3a1cf8464d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171b43dac457484ba2d839302c52cd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "classifier.weight                          | MISSING    | \n",
      "classifier.bias                            | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72804dbd-0e3a-4b22-b1b2-b44f0fa71db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3553b414-d42b-40cc-917d-c6b87011bbcd",
   "metadata": {},
   "source": [
    "## Training - Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c24f0133-a063-4aa0-a2fc-9b90477912fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "'''\n",
    "    When you pass a tokenizer as the processing_class, the default data_collator \n",
    "    used by the Trainer will be a DataCollatorWithPadding. You can skip the \n",
    "    data_collator=data_collator line in this case, but we included it here \n",
    "    to show you this important part of the processing pipeline\n",
    "\n",
    "    COOKING RECIPEIES \n",
    "    https://huggingface.co/learn/cookbook/en/fine_tuning_code_llm_on_single_gpu\n",
    "\n",
    "'''\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator, # to do dynamic padding. \n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113591b-574e-41f4-b77e-379d2b70ec94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f36f69c-cd12-45a5-818d-3a328d06dc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 20:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.601757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.414775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3bee981d4b421594a7c535533ede23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60afb137d0ac43aaa0105915d77ed0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329153ad7ae44071a05537edc7846b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 36s, sys: 32.9 s, total: 19min 9s\n",
      "Wall time: 20min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.45004071337465107, metrics={'train_runtime': 1208.6284, 'train_samples_per_second': 9.105, 'train_steps_per_second': 1.139, 'total_flos': 405114969714960.0, 'train_loss': 0.45004071337465107, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312fab2c-53e6-4164-ad8f-2700ff55519e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b4aaa-31a1-46e9-8099-c747f22c2bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d50a601-72a2-4285-bbb2-ac624d91142d",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b113a84-d674-47b8-b456-f4565c42d58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbc93c76-c4c0-47ed-88f9-ba0daf0ab1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1337ad7-9071-432c-b986-347c4883d032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2a23cd66f4495f8e1bbdcafb12f899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8333333333333334, 'f1': 0.8815331010452961}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c459994-7a90-4c0e-bd92-7f1114d02ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6d3b9a9-7ce1-4588-8b60-70b92cbf8f85",
   "metadata": {},
   "source": [
    "## A Full training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c149560-a082-41ca-a2f7-da7499784b2d",
   "metadata": {},
   "source": [
    "https://huggingface.co/learn/llm-course/chapter3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50fdfc9-351d-4676-986b-4f84792b213c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce89755-1b0d-46b7-9e32-6810d97cdbef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
